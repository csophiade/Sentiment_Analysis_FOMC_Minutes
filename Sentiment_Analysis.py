# -*- coding: utf-8 -*-
"""IBC SNT G8

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WbwqYqGjondUmmUPt10sMExB2YUidX5H
"""

from __future__ import division
from bs4 import BeautifulSoup
from collections import Counter
from textblob import TextBlob as tb
import math
import spacy
from spacy.matcher import PhraseMatcher
import re
import nltk

nltk.download('punkt_tab')
import requests


def analyze_sentiment(doc):
    matches = matcher(doc)
    hawkish_count = 0
    dovish_count = 0

    for match_id, start, end in matches:
        span = doc[start:end]
        if nlp.vocab.strings[match_id] == "HAWKISH":
            hawkish_count += 1
        elif nlp.vocab.strings[match_id] == "DOVISH":
            dovish_count += 1

    return hawkish_count, dovish_count


def tf(word, blob):
    return blob.words.count(word) / len(blob.words)


def n_containing(word, bloblist):
    return sum(1 for blob in bloblist if word in blob.words)


def idf(word, bloblist):
    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))


def tfidf(word, blob, bloblist):
    return tf(word, blob) * idf(word, bloblist)


# Base URL for the historical FOMC page
base_url = "https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"

# Request to get the page content
r = requests.get(base_url)
soup = BeautifulSoup(r.text, 'html.parser')

# Find all years listed on the page
all_years = soup.find_all('div', class_="col-xs-6 col-sm-4")

# List to store all FOMC minutes links
all_minutes_links = []

# Loop through each year link
for year_div in all_years:
    for link in year_div.find_all('a', href=True):
        if link:
            # Build the full URL for the year's page
            year_url = 'https://www.federalreserve.gov' + link['href']

            # Request page for that year
            year_r = requests.get(year_url)
            year_soup = BeautifulSoup(year_r.content, 'html.parser')

            # Find all links in the year-specific page
            year_page_links = year_soup.find_all('a', href=True)

            # Filter for links containing the word 'minutes' and ending with '.htm' and NEW FILTER (dont want before 2000)
            for year_link in year_page_links:
                href = year_link['href']
                if 'minutes' in href.lower() and href.endswith('.htm'):
                    # Extract year from the href using regex
                    match = re.search(r'(\d{4})', href)
                    if match and int(match.group(1)) >= 2014:
                        # Append the full URL to the list of minutes links
                        all_minutes_links.append('https://www.federalreserve.gov' + href)


# Print all collected FOMC minutes links
# for minutes_link in all_minutes_links:
# print(minutes_link)

word_count_list = []
# hawkish words list
hawkish_phrases = ["tightening financial conditions", "inflationary pressures", "rate hikes",
                   "restrictive monetary policy", "overheating economy",
                   "rising wages", "increased demand", "stabilizing inflation", "inflation above target",
                   "worries over inflation",
                   "gradual increases in rates", "rising core inflation", "normalizing rates", "preemptive tightening",
                   "aggressive rate adjustments",
                   "avoiding excessive growth", "restricting liquidity", "controlling inflation",
                   "expanding policy tightening",
                   "inflation control measures", "inflation expectations rising", "interest rate increases",
                   "confronting inflationary risk",
                   "sustained policy tightening", "monetary tightening path", "economic overheating risks",
                   "keeping inflation in check",
                   "reducing inflation expectations", "anticipated interest rate hikes", "upside risk to inflation",
                   "rising economic activity",
                   "inflation-targeting approach", "low inflation", "reducing liquidity"]
dovish_phrases = ["accommodative monetary policy", "supporting economic growth", "maintaining low interest rates",
                  "easing financial conditions",
                  "reduced inflationary pressures", "slower economic expansion", "below target inflation",
                  "economic slowdown", "low inflation",
                  "gradual rate increases", "downside risks to growth", "weak economic activity",
                  "moderating inflation", "encouraging investment",
                  "softening inflationary pressures", "risk of economic stagnation", "further rate cuts",
                  "stimulative policy",
                  "low inflation environment", "slow recovery", "patient approach",
                  "allowing for economic accommodation", "deflation risks",
                  "low growth potential", "monetary accommodation", "inflation expectations subdued",
                  "lower inflation risks",
                  "economic risks remain low", "continued policy accommodation", "long-term growth concerns",
                  "no immediate inflationary threat",
                  "muted inflation pressures", "slower growth", "easing", "accommodative policy"]
hawkish_size = len(hawkish_phrases)
dovish_size = len(dovish_phrases)
# dovish words list
# NEW CODE --- each link analysis
# loop through each link
bloblist = []
nlp = spacy.load("en_core_web_sm")

# Define hawkish and dovish patterns
hawkish_patterns = list(nlp.pipe(hawkish_phrases))
dovish_patterns = list(nlp.pipe(dovish_phrases))

# Initialize PhraseMatcher
matcher = PhraseMatcher(nlp.vocab)
matcher.add("HAWKISH", None, *hawkish_patterns)
matcher.add("DOVISH", None, *dovish_patterns)

sentiment = []
dates = []

for minutes_link in all_minutes_links:
    # url for the minutes sheet
    minutes_r = requests.get(minutes_link)
    minutes_soup = BeautifulSoup(minutes_r.content, 'html.parser')
    # print(minutes_soup.prettify())

    # get all the etext
    page_text = minutes_soup.get_text(separator=" ").lower()

    doc = nlp(page_text)
    hawkish_count, dovish_count = analyze_sentiment(doc)

    word = ""
    dash = "-"
    y = 0
    for letter in minutes_link:
        if letter.isnumeric():
            y = y + 1
            word = word + letter
            if (y == 4 or y == 6):
                word = word + dash
    dates.append(word)

    # print(word, end =": ")
    # print(f"Hawkish count: {hawkish_count}", end = " ")
    # print(f"Dovish count: {dovish_count}")
    # divide by the total number as some docs only have 2 total phrases whereas others have 17
    if hawkish_count + dovish_count > 0:
        sentiment.append(float((dovish_count - hawkish_count) / (hawkish_count + dovish_count)))
    else:
        sentiment.append(float(0))

# normalish the results
#mini = float(min(sentiment))
#maxi = float(max(sentiment))

i = 0
for date in dates:
    print(dates[i], end=": ")
    print(f"{sentiment[i]:.2f}")
    i = i + 1

    # TEXT BLOB ANALYSIS -------------------------------
    # blob = tb(page_text)
    # bloblist.append(blob)

    # OWN SENTIMENT ANALYSIS ---------------------------------------------
    # search through list and find occurences
'''
    for hawkish in hawkish_list:
      word_count_list.append(page_text.count(hawkish))### TARGET WORDS

    word = ""
    dash = "-"
    y = 0

    for letter in minutes_link:
      if letter.isnumeric():
          y = y + 1
          word = word + letter
          if (y==4 or y==6):
            word = word + dash

    print(word, end =": ")

    for i in range(1,hawkish_size+1):
      print(hawkish_list[i-1], "-", word_count_list[i-1],end =", ")

    print()
    '''

'''
  for i, blob in enumerate(bloblist):
    print("Top words in document {}".format(i + 1))
    scores = {word.lower(): tfidf(word.lower(), blob, bloblist) for word in blob.words}
    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)
    for word, score in sorted_words[:3]:
        print("\tWord: {}, TF-IDF: {}".format(word, round(score, 5)))
'''
